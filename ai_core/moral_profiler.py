# -*- coding: utf-8 -*-
"""
åŠ¨æ€é“å¾·ç”»åƒæ¨¡å— (Moral Profiler) - å·²é›†æˆDeepSeek LLMæ¡†æž¶

è¯¥æ¨¡å—è´Ÿè´£æž„å»ºå’Œç»´æŠ¤å¯¹çŽ©å®¶é“å¾·å€¾å‘çš„ç”»åƒã€‚
å®ƒé‡‡ç”¨å¤šæ¨¡æ€è¾“å…¥ï¼ˆè¡Œä¸ºé¥æµ‹å’Œå¯¹è¯åˆ†æžï¼‰ï¼Œä»¥åŠ¨æ€ç†è§£çŽ©å®¶çš„æ„å›¾å’Œä»·å€¼è§‚ã€‚
"""

import os
import json
from typing import Dict, Any, List

# --- DeepSeek LLM é›†æˆè¯´æ˜Ž (æœ€ç»ˆç‰ˆ) ---
# 1. å®‰è£…ä¾èµ–: pip install openai
#    (æ˜¯çš„ï¼Œæˆ‘ä»¬ä½¿ç”¨openaiåº“æ¥è°ƒç”¨DeepSeekçš„å…¼å®¹API)
# 2. è®¾ç½®çŽ¯å¢ƒå˜é‡: åˆ›å»ºä¸€ä¸ªåä¸º DEEPSEEK_API_KEY çš„çŽ¯å¢ƒå˜é‡ã€‚
# 3. æ¿€æ´»LLM: åœ¨ run_example.py ä¸­è®¾ç½® use_llm=True
# -------------------------------------

try:
    # æˆ‘ä»¬ä½¿ç”¨OpenAIçš„åº“æ¥è°ƒç”¨DeepSeek
    from openai import OpenAI
except ImportError:
    OpenAI = None

class MoralProfiler:
    """
    çŽ©å®¶é“å¾·ç”»åƒæž„å»ºå™¨ï¼Œå†…ç½®LLMåˆ†æžèƒ½åŠ›ã€‚
    """

    def __init__(self, dialogue_update_factor: float = 0.2, use_llm: bool = False):
        self.dialogue_update_factor = dialogue_update_factor
        # æœ€ç»ˆä¿®å¤AttributeError: ç¡®ä¿è¿™ä¸ªå±žæ€§è¢«æ­£ç¡®å‘½åå’Œåˆå§‹åŒ–
        self.player_moral_profile: Dict[str, float] = {
            "harm_care": 0.5,
            "fairness_reciprocity": 0.5,
            "ingroup_loyalty": 0.5,
            "authority_respect": 0.5,
            "purity_sanctity": 0.5,
        }
        self.behavior_history: List[Any] = []
        self.dialogue_history: List[str] = []

        self.use_llm = use_llm
        self.llm_client = None

        if self.use_llm and OpenAI and os.getenv("DEEPSEEK_API_KEY"):
            print("âœ… æ£€æµ‹åˆ°DeepSeek APIå¯†é’¥ï¼ŒLLMæ¨¡å¼å·²æ¿€æ´»ã€‚")
            # æœ€ç»ˆä¿®å¤ï¼šéµå¾ªå®˜æ–¹æ–‡æ¡£ï¼Œä½¿ç”¨OpenAIå®¢æˆ·ç«¯å¹¶æŒ‡å®šDeepSeekçš„base_url
            self.llm_client = OpenAI(
                api_key=os.getenv("DEEPSEEK_API_KEY"),
                base_url="https://api.deepseek.com/v1"
            )
        elif self.use_llm:
            print("âš ï¸ è­¦å‘Š: å·²è¯·æ±‚ä½¿ç”¨LLMï¼Œä½†æœªæ‰¾åˆ°OpenAIåº“æˆ–DeepSeek APIå¯†é’¥ã€‚å°†å›žé€€åˆ°æ¨¡æ‹Ÿæ¨¡å¼ã€‚")
            self.use_llm = False

    def _build_llm_prompt(self, dialogue: str) -> str:
        """
        æž„å»ºç”¨äºŽåˆ†æžçŽ©å®¶å¯¹è¯çš„LLMæç¤ºï¼ˆPromptï¼‰ã€‚
        """
        moral_dimensions = list(self.player_moral_profile.keys())
        return f"""You are a moral psychology analyst. Your task is to analyze the following user dialogue from a video game and determine which moral foundations it expresses. The user said: "{dialogue}"

Please respond with a JSON object only, with no other text. The JSON object should have keys from the list {moral_dimensions}. For each key, provide a float value from 0.0 to 1.0 representing the confidence that the user's dialogue expresses that moral dimension. If a dimension is not expressed, set its value to 0.0."""

    def _analyze_dialogue_with_llm(self, dialogue: str) -> Dict[str, float]:
        """
        ä½¿ç”¨çœŸå®žçš„LLM APIåˆ†æžå¯¹è¯ã€‚çŽ°åœ¨ä½¿ç”¨DeepSeekã€‚
        """
        if not self.use_llm or not self.llm_client:
            return self._analyze_dialogue_simulation(dialogue)

        print(f"   ðŸ§  [DeepSeekçœŸå®žè°ƒç”¨] æ­£åœ¨å‘DeepSeek APIå‘é€è¯·æ±‚...")
        prompt = self._build_llm_prompt(dialogue)

        try:
            # æœ€ç»ˆä¿®å¤ï¼šä½¿ç”¨æ ‡å‡†çš„client.chat.completions.createæ–¹æ³•
            response = self.llm_client.chat.completions.create(
                model="deepseek-chat",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"} 
            )
            analysis_str = response.choices[0].message.content
            analysis_result = json.loads(analysis_str)
            print(f"   ðŸ§  [DeepSeekçœŸå®žè°ƒç”¨] æ”¶åˆ°å¹¶è§£æžäº†åˆ†æžç»“æžœ: {analysis_result}")
            return analysis_result
        except Exception as e:
            print(f"   âŒ [DeepSeekçœŸå®žè°ƒç”¨] APIè°ƒç”¨å¤±è´¥: {e}")
            print(f"   âš ï¸ å°†å›žé€€åˆ°æ¨¡æ‹Ÿæ¨¡å¼è¿›è¡Œæœ¬æ¬¡åˆ†æžã€‚")
            return self._analyze_dialogue_simulation(dialogue)

    def _analyze_dialogue_simulation(self, dialogue: str) -> Dict[str, float]:
        """
        (æ¨¡æ‹Ÿ) ä½¿ç”¨å…³é”®è¯åˆ†æžå¯¹è¯ï¼Œä½œä¸ºLLMä¸å¯ç”¨æ—¶çš„åŽå¤‡æ–¹æ¡ˆã€‚
        """
        print(f"   ðŸ§  [æ¨¡æ‹Ÿæ¨¡å¼] æ­£åœ¨åˆ†æž: '{dialogue}'")
        dialogue = dialogue.lower()
        analysis_result = {}
        if "suffer" in dialogue or "help them" in dialogue or "pity" in dialogue:
            analysis_result["harm_care"] = 0.8
        if "fair" in dialogue or "deserve" in dialogue:
            analysis_result["fairness_reciprocity"] = 0.7
        print(f"   ðŸ§  [æ¨¡æ‹Ÿæ¨¡å¼] åˆ†æžç»“æžœ: {analysis_result}")
        return analysis_result

    def process_behavior_event(self, event: Any):
        self.behavior_history.append(event)
        print(f"å¤„ç†è¡Œä¸ºäº‹ä»¶: {event}")

    def process_dialogue_event(self, dialogue: str):
        self.dialogue_history.append(dialogue)
        print(f"å¤„ç†å¯¹è¯äº‹ä»¶: '{dialogue}'")
        
        analysis = self._analyze_dialogue_with_llm(dialogue)

        for moral_dimension, confidence in analysis.items():
            if moral_dimension in self.player_moral_profile:
                current_value = self.player_moral_profile[moral_dimension]
                adjustment = (1.0 - current_value) * confidence * self.dialogue_update_factor
                new_value = current_value + adjustment
                self.player_moral_profile[moral_dimension] = min(new_value, 1.0)
                print(f"   - ç”»åƒæ›´æ–°: '{moral_dimension}' ä»Ž {current_value:.2f} -> {self.player_moral_profile[moral_dimension]:.2f}")

    def get_player_profile(self) -> Dict[str, float]:
        # æœ€ç»ˆä¿®å¤AttributeError: ç¡®ä¿è¿”å›žæ­£ç¡®çš„å±žæ€§
        return self.player_moral_profile

    def __repr__(self) -> str:
        return f"MoralProfiler(Profile={self.player_moral_profile})"
